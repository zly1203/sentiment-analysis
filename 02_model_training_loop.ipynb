{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f70dffe-0c53-4f54-81e7-fd693631a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2 gold file(s)...\n",
      "Test lockbox created: 99 rows\n",
      "\n",
      "Data preparation complete:\n",
      "  Test set:  99 rows (locked)\n",
      "  Train set: 883 rows (ready)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Phase 1: Data Preparation\n",
    "#\n",
    "# Loads all gold-standard CSVs, deduplicates by text (keeping the\n",
    "# latest human correction), then manages the test lockbox:\n",
    "#   - First run:  create initial 90/10 split\n",
    "#   - Later runs: append 10% of NEW data only (never reshuffle)\n",
    "#\n",
    "# \"New data\" = texts not in test lockbox AND not in previous staging.\n",
    "# Training set = ALL gold data minus the full test lockbox (not just\n",
    "# new data), so the model always trains on the complete history.\n",
    "#\n",
    "# Safe to re-run: if no new data exists, test lockbox is unchanged\n",
    "# and training set is rebuilt identically from gold minus test.\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import DIRS\n",
    "\n",
    "STAGING_PATH = f\"{DIRS['staging']}/ready_for_training.csv\"\n",
    "TEST_PATH = f\"{DIRS['test_lockbox']}/final_test_set.csv\"\n",
    "\n",
    "\n",
    "def load_and_standardize(filepath):\n",
    "    \"\"\"Load a gold CSV and normalize to [text, label] schema.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    if 'gpt_label' in df.columns and 'label' not in df.columns:\n",
    "        df = df.rename(columns={'gpt_label': 'label'})\n",
    "    try:\n",
    "        return df[['text', 'label']]\n",
    "    except KeyError:\n",
    "        print(f\"  SKIP: {os.path.basename(filepath)} (missing 'text' or 'label' column)\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# -- Load all gold-standard files --\n",
    "files = glob.glob(f\"{DIRS['gold']}/*.csv\")\n",
    "if not files:\n",
    "    raise ValueError(f\"No files in {DIRS['gold']}/. Run Notebook 01 first.\")\n",
    "\n",
    "print(f\"Loading {len(files)} gold file(s)...\")\n",
    "df_list = [load_and_standardize(f) for f in files]\n",
    "full_dataset = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Clean: drop NaN rows, normalize labels to lowercase\n",
    "full_dataset = full_dataset.dropna(subset=['text', 'label'])\n",
    "full_dataset['label'] = full_dataset['label'].str.lower()\n",
    "\n",
    "# Deduplicate by text (keep latest = most recent human correction wins)\n",
    "before_dedup = len(full_dataset)\n",
    "full_dataset = full_dataset.drop_duplicates(subset=['text'], keep='last')\n",
    "after_dedup = len(full_dataset)\n",
    "if before_dedup > after_dedup:\n",
    "    print(f\"Dedup: {before_dedup} -> {after_dedup} (removed {before_dedup - after_dedup} duplicates)\")\n",
    "\n",
    "# -- Test lockbox: create once, then append-only (never reshuffle existing rows) --\n",
    "if os.path.exists(TEST_PATH):\n",
    "    existing_test = pd.read_csv(TEST_PATH)\n",
    "    print(f\"Test lockbox exists: {len(existing_test)} rows (locked)\")\n",
    "\n",
    "    # \"New\" = not in test lockbox AND not in previous training staging.\n",
    "    # This prevents re-runs from treating old training rows as new data\n",
    "    # and repeatedly moving them into the test lockbox.\n",
    "    known_texts = set(existing_test['text'].tolist())\n",
    "    if os.path.exists(STAGING_PATH):\n",
    "        known_texts |= set(pd.read_csv(STAGING_PATH)['text'].tolist())\n",
    "\n",
    "    new_data = full_dataset[~full_dataset['text'].isin(known_texts)]\n",
    "    print(f\"New data: {len(new_data)} rows\")\n",
    "\n",
    "    if len(new_data) > 0:\n",
    "        _, new_test = train_test_split(new_data, test_size=0.1, random_state=42)\n",
    "        updated_test = pd.concat([existing_test, new_test], ignore_index=True)\n",
    "        updated_test.to_csv(TEST_PATH, index=False)\n",
    "        print(f\"Test lockbox updated: {len(existing_test)} -> {len(updated_test)} (+{len(new_test)} appended)\")\n",
    "    else:\n",
    "        updated_test = existing_test\n",
    "        print(\"No new data for test lockbox.\")\n",
    "\n",
    "    # Training set = ALL gold minus the full test lockbox.\n",
    "    # This ensures the model always trains on complete history, not just new rows.\n",
    "    train_df = full_dataset[~full_dataset['text'].isin(updated_test['text'])]\n",
    "\n",
    "else:\n",
    "    # First run: create initial 90/10 split\n",
    "    train_df, test_df = train_test_split(full_dataset, test_size=0.1, random_state=42)\n",
    "    test_df.to_csv(TEST_PATH, index=False)\n",
    "    print(f\"Test lockbox created: {len(test_df)} rows\")\n",
    "\n",
    "# -- Guard: abort early if nothing to train on --\n",
    "if train_df.empty:\n",
    "    raise ValueError(\n",
    "        \"Training set is empty â€” all data is already in the test lockbox. \"\n",
    "        \"Add new labeled data to 04_gold_standard/ before retraining.\"\n",
    "    )\n",
    "\n",
    "train_df.to_csv(STAGING_PATH, index=False)\n",
    "\n",
    "print(f\"\\nData preparation complete:\")\n",
    "print(f\"  Test set:  {len(pd.read_csv(TEST_PATH))} rows (locked)\")\n",
    "print(f\"  Train set: {len(train_df)} rows (ready)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c712488-e137-412c-9468-8cf31eae2e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddb5c47d01741a999cdd8ba33c5b4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5467fe748f074945b6281760c1db9276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7968006d4f34405991332e7ce2053d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc77e3152664eaeaec69390401ee897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Train: 883 rows, Test: 99 rows\n",
      "Model will be saved to: ./06_models/v_20260218_132249\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Phase 2: Tokenization\n",
    "#\n",
    "# Reads training and test CSVs, maps string labels to integers\n",
    "# (with .str.lower() to prevent silent drops from case mismatch),\n",
    "# then tokenizes text with the DistilBERT tokenizer.\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import AutoTokenizer\n",
    "from config import DIRS, MODEL_NAME, MAX_LENGTH, LABEL_MAP\n",
    "\n",
    "TODAY = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "NOW = datetime.datetime.now().strftime(\"%H%M%S\")\n",
    "OUTPUT_DIR = f\"{DIRS['models']}/v_{TODAY}_{NOW}\"\n",
    "\n",
    "# -- Load data --\n",
    "df_train = pd.read_csv(f\"{DIRS['staging']}/ready_for_training.csv\")\n",
    "df_test = pd.read_csv(f\"{DIRS['test_lockbox']}/final_test_set.csv\")\n",
    "\n",
    "# -- Map labels to integers (normalize case first to prevent silent NaN drops) --\n",
    "df_train['label'] = df_train['label'].str.lower().map(LABEL_MAP)\n",
    "df_test['label'] = df_test['label'].str.lower().map(LABEL_MAP)\n",
    "\n",
    "df_train = df_train.dropna(subset=['label', 'text'])\n",
    "df_test = df_test.dropna(subset=['label', 'text'])\n",
    "\n",
    "df_train['label'] = df_train['label'].astype(int)\n",
    "df_test['label'] = df_test['label'].astype(int)\n",
    "\n",
    "# -- Build HuggingFace datasets with ClassLabel type --\n",
    "train_ds = Dataset.from_pandas(df_train, preserve_index=False)\n",
    "test_ds = Dataset.from_pandas(df_test, preserve_index=False)\n",
    "\n",
    "c_label = ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'])\n",
    "train_ds = train_ds.cast_column(\"label\", c_label)\n",
    "test_ds = test_ds.cast_column(\"label\", c_label)\n",
    "\n",
    "# -- Tokenize text --\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize, batched=True)\n",
    "tokenized_test = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
    "\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")\n",
    "\n",
    "print(f\"Tokenization complete. Train: {len(df_train)} rows, Test: {len(df_test)} rows\")\n",
    "print(f\"Model will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e81c95bd-71da-4a0e-869c-d5308bc840ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/1w/cgxpx8gj4p1czd_4ly9shd8h0000gq/T/ipykernel_77461/870445043.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started... Target: ./06_models/v_20260218_132249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zz/Desktop/sentiment-analysis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='333' max='333' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [333/333 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.748451</td>\n",
       "      <td>0.747475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.497100</td>\n",
       "      <td>0.692904</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.676605</td>\n",
       "      <td>0.767677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zz/Desktop/sentiment-analysis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/zz/Desktop/sentiment-analysis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/zz/Desktop/sentiment-analysis/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluation Report\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.744     0.800     0.771        40\n",
      "     neutral      0.864     0.594     0.704        32\n",
      "    positive      0.735     0.926     0.820        27\n",
      "\n",
      "    accuracy                          0.768        99\n",
      "   macro avg      0.781     0.773     0.765        99\n",
      "weighted avg      0.780     0.768     0.763        99\n",
      "\n",
      "Confusion Matrix (rows=true, cols=predicted):\n",
      "               PRED:negative  PRED:neutral  PRED:positive\n",
      "TRUE:negative             32             3              5\n",
      "TRUE:neutral               9            19              4\n",
      "TRUE:positive              2             0             25\n",
      "\n",
      "Cleaned up temp checkpoints: ./results_temp/\n",
      "--------------------------------------------------\n",
      "Training complete!\n",
      "  Accuracy:  76.77%\n",
      "  Macro F1:  76.48%\n",
      "  Model:     ./06_models/v_20260218_132249\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Phase 3: Model Training & Evaluation\n",
    "#\n",
    "# Fine-tunes DistilBERT on the prepared dataset, evaluates on the\n",
    "# locked test set, saves model + model_meta.json with full metrics\n",
    "# (accuracy, macro F1, per-class precision/recall/F1, confusion matrix),\n",
    "# then cleans up intermediate checkpoints to prevent disk bloat.\n",
    "# ==========================================\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "\n",
    "# -- Load pre-trained model --\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    id2label={0: \"negative\", 1: \"neutral\", 2: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"neutral\": 1, \"positive\": 2},\n",
    ")\n",
    "\n",
    "# -- Training config --\n",
    "TEMP_DIR = \"./results_temp\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TEMP_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# -- Train --\n",
    "print(f\"Training started... Target: {OUTPUT_DIR}\")\n",
    "trainer.train()\n",
    "\n",
    "# -- Save final model + tokenizer --\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# -- Full evaluation on test lockbox --\n",
    "label_names = ['negative', 'neutral', 'positive']\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "pred_ids = predictions.predictions.argmax(-1)\n",
    "true_ids = predictions.label_ids\n",
    "\n",
    "# Per-class metrics\n",
    "report = classification_report(true_ids, pred_ids, target_names=label_names, output_dict=True)\n",
    "cm = confusion_matrix(true_ids, pred_ids).tolist()\n",
    "\n",
    "# Print readable report\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Evaluation Report\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(true_ids, pred_ids, target_names=label_names, digits=3))\n",
    "\n",
    "print(\"Confusion Matrix (rows=true, cols=predicted):\")\n",
    "import pandas as _pd\n",
    "cm_df = _pd.DataFrame(cm,\n",
    "    index=[f\"TRUE:{l}\" for l in label_names],\n",
    "    columns=[f\"PRED:{l}\" for l in label_names])\n",
    "print(cm_df)\n",
    "\n",
    "# -- Save model_meta.json with full metrics --\n",
    "meta = {\n",
    "    \"model_name\": os.path.basename(OUTPUT_DIR),\n",
    "    \"accuracy\": round(report[\"accuracy\"], 4),\n",
    "    \"macro_f1\": round(report[\"macro avg\"][\"f1-score\"], 4),\n",
    "    \"weighted_f1\": round(report[\"weighted avg\"][\"f1-score\"], 4),\n",
    "    \"per_class\": {\n",
    "        name: {\n",
    "            \"precision\": round(report[name][\"precision\"], 4),\n",
    "            \"recall\": round(report[name][\"recall\"], 4),\n",
    "            \"f1\": round(report[name][\"f1-score\"], 4),\n",
    "            \"support\": report[name][\"support\"],\n",
    "        }\n",
    "        for name in label_names\n",
    "    },\n",
    "    \"confusion_matrix\": cm,\n",
    "    \"train_samples\": len(df_train),\n",
    "    \"test_samples\": len(df_test),\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"created_at\": datetime.datetime.now().isoformat(),\n",
    "}\n",
    "meta_path = f\"{OUTPUT_DIR}/model_meta.json\"\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "# -- Clean up intermediate checkpoints to save disk space --\n",
    "if os.path.exists(TEMP_DIR):\n",
    "    shutil.rmtree(TEMP_DIR)\n",
    "    print(f\"\\nCleaned up temp checkpoints: {TEMP_DIR}/\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"  Accuracy:  {meta['accuracy']:.2%}\")\n",
    "print(f\"  Macro F1:  {meta['macro_f1']:.2%}\")\n",
    "print(f\"  Model:     {OUTPUT_DIR}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663a6bc-cfb4-489d-9183-5a5184435e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
