{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1007fa-59e1-45de-ae8d-792902f70911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline directories:\n",
      "  [         raw] ./01_raw_inbox/\n",
      "  [     staging] ./02_staging/\n",
      "  [      review] ./03_manual_review/\n",
      "  [        gold] ./04_gold_standard/\n",
      "  [test_lockbox] ./05_test_lockbox/\n",
      "  [      models] ./06_models/\n",
      "  [   workspace] ./07_daily_workspace/\n",
      "  [     reports] ./08_client_reports/\n",
      "  [    baseline] ./09_internal_baseline/\n",
      "\n",
      "All 9 directories ready.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 1: Verify Pipeline Directory Structure\n",
    "#\n",
    "# Imports shared config, which auto-creates all directories on import.\n",
    "# This cell serves as a sanity check before running the pipeline.\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "from config import DIRS\n",
    "\n",
    "print(\"Pipeline directories:\")\n",
    "for key, path in DIRS.items():\n",
    "    print(f\"  [{key:>12}] {path}/\")\n",
    "print(f\"\\nAll {len(DIRS)} directories ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44aedbd2-f012-4fa1-bc6a-cff447f4ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading [cold_start]: raw.csv ...\n",
      "Labeling with GPT-4o-mini ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [09:20<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Auto-pass  -> gold:     531\n",
      "Conflict   -> review:   451\n",
      "Baseline   -> internal: 982 rows\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 2: Cold-Start Ingest — GPT Labeling & Conflict Detection\n",
    "#\n",
    "# Reads raw data, labels each row with GPT-4o-mini, then splits:\n",
    "#   - Match  (GPT == existing sentiment) → 04_gold_standard (auto-approved)\n",
    "#   - Conflict (GPT != existing)         → 03_manual_review (needs human)\n",
    "#   - Full baseline                      → 09_internal_baseline (internal ref)\n",
    "#\n",
    "# GPT API failures are counted and reported — rows that fail\n",
    "# default to \"neutral\" but a warning is printed at the end.\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from config import DIRS\n",
    "\n",
    "load_dotenv()\n",
    "TODAY = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# -- Config --\n",
    "PROJECT_NAME = \"cold_start\"\n",
    "TARGET_FILENAME = \"raw.csv\"\n",
    "BATCH_LABEL = \"cold_start\"\n",
    "\n",
    "INPUT_PATH = f\"{DIRS['raw']}/{PROJECT_NAME}/{TARGET_FILENAME}\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Input not found: {INPUT_PATH}. \"\n",
    "        f\"Create {DIRS['raw']}/{PROJECT_NAME}/ and add {TARGET_FILENAME}.\"\n",
    "    )\n",
    "\n",
    "print(f\"Reading [{PROJECT_NAME}]: {TARGET_FILENAME} ...\")\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Add stable row IDs and batch tag for traceability\n",
    "df[\"global_uuid\"] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "df[\"batch_source\"] = BATCH_LABEL\n",
    "\n",
    "# -- GPT Labeling --\n",
    "print(\"Labeling with GPT-4o-mini ...\")\n",
    "tqdm.pandas()\n",
    "\n",
    "VALID_LABELS = {\"positive\", \"negative\", \"neutral\"}\n",
    "_gpt_error_count = 0\n",
    "\n",
    "def get_gpt_score(text):\n",
    "    \"\"\"Call GPT-4o-mini for sentiment. Tracks errors instead of silently swallowing them.\"\"\"\n",
    "    global _gpt_error_count\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are an expert sentiment analyst. \"\n",
    "                        \"Analyze the text for subtle nuances (e.g., sarcasm, nostalgia, mixed feelings).\\n\\n\"\n",
    "                        \"MANDATORY MAPPING RULES:\\n\"\n",
    "                        \"1. Sarcasm / Complaint / Regret -> 'Negative'\\n\"\n",
    "                        \"2. Praise / Nostalgia / Excitement -> 'Positive'\\n\"\n",
    "                        \"3. Facts / Questions / Mixed / Unclear -> 'Neutral'\\n\\n\"\n",
    "                        \"OUTPUT FORMAT: Strictly ONE word only from: [Positive, Negative, Neutral].\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        raw_label = response.choices[0].message.content.strip().lower().replace(\".\", \"\")\n",
    "        return raw_label if raw_label in VALID_LABELS else \"neutral\"\n",
    "    except Exception as e:\n",
    "        _gpt_error_count += 1\n",
    "        if _gpt_error_count <= 3:\n",
    "            print(f\"  [GPT ERROR #{_gpt_error_count}] {type(e).__name__}: {e}\")\n",
    "        return \"neutral\"\n",
    "\n",
    "df_run = df.copy()\n",
    "df_run[\"gpt_label\"] = df_run[\"text\"].progress_apply(get_gpt_score)\n",
    "df_run[\"gpt_label\"] = df_run[\"gpt_label\"].str.lower()\n",
    "\n",
    "# Surface GPT failures clearly so the user knows the data may be unreliable\n",
    "if _gpt_error_count > 0:\n",
    "    print(f\"\\n*** WARNING: {_gpt_error_count}/{len(df_run)} GPT calls failed \"\n",
    "          f\"(defaulted to 'neutral'). Check API key / quota. ***\\n\")\n",
    "\n",
    "# -- Split by agreement: match → gold, conflict → review --\n",
    "def check_conflict(row):\n",
    "    original = str(row[\"sentiment\"]).lower().strip()\n",
    "    gpt = str(row[\"gpt_label\"]).lower().strip()\n",
    "    return \"auto_pass\" if original == gpt else \"needs_review\"\n",
    "\n",
    "df_run[\"status\"] = df_run.apply(check_conflict, axis=1)\n",
    "df_pass = df_run[df_run[\"status\"] == \"auto_pass\"]\n",
    "df_review = df_run[df_run[\"status\"] == \"needs_review\"]\n",
    "\n",
    "# -- Write outputs --\n",
    "now = datetime.datetime.now().isoformat()\n",
    "\n",
    "# Gold: auto-approved rows (standardized schema: text, label, gold_origin, created_at)\n",
    "gold_pass = df_pass[['text', 'gpt_label']].copy()\n",
    "gold_pass.rename(columns={'gpt_label': 'label'}, inplace=True)\n",
    "gold_pass['gold_origin'] = 'cold_start_auto'\n",
    "gold_pass['created_at'] = now\n",
    "gold_pass.to_csv(f\"{DIRS['gold']}/{TODAY}_{BATCH_LABEL}_auto.csv\", index=False)\n",
    "\n",
    "# Review: conflict rows (full columns preserved for human inspection)\n",
    "df_review.to_csv(f\"{DIRS['review']}/{TODAY}_{BATCH_LABEL}_review.csv\", index=False)\n",
    "\n",
    "# Baseline: complete dataset for internal reference (not client-facing)\n",
    "baseline_path = f\"{DIRS['baseline']}/{TODAY}_{BATCH_LABEL}_{len(df_run)}rows.csv\"\n",
    "df_run.to_csv(baseline_path, index=False)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(f\"Auto-pass  -> gold:     {len(df_pass)}\")\n",
    "print(f\"Conflict   -> review:   {len(df_review)}\")\n",
    "print(f\"Baseline   -> internal: {len(df_run)} rows\")\n",
    "if _gpt_error_count > 0:\n",
    "    print(f\"GPT errors (defaulted): {_gpt_error_count}\")\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920e768-497a-4a30-9342-a7981b5a6575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sentiment-analysis)",
   "language": "python",
   "name": "sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
