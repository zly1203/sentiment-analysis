{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df803b87-4afb-4612-b4c9-bbbfaf47fe89",
   "metadata": {},
   "outputs": [],
   "source": "# ==========================================\n# STAGE 1: Predict & Triage\n#\n# Loads the best model (selected by accuracy from model_meta.json),\n# runs inference on new data, then splits by confidence threshold:\n#   - High confidence (>= threshold) → 07_daily_workspace (auto-pass)\n#   - Low confidence  (<  threshold) → 03_manual_review (needs human)\n# ==========================================\n\nimport json\nimport pandas as pd\nimport glob\nimport os\nimport datetime\nfrom transformers import pipeline\nfrom config import DIRS, MAX_LENGTH\n\n# -- Config (edit before each run) --\nCURRENT_PROJECT = \"cold_start\"\nCONFIDENCE_THRESHOLD = 0.7\nTODAY = datetime.date.today().strftime(\"%Y%m%d\")\n\nprint(f\"Project: [{CURRENT_PROJECT}]\")\n\n# -- Load input data --\ninput_folder = f\"{DIRS['raw']}/{CURRENT_PROJECT}\"\nfiles = glob.glob(f\"{input_folder}/*.csv\")\n\nif not files:\n    raise FileNotFoundError(f\"No CSV found in {input_folder}/. Add data before running.\")\n\nraw_file = sorted(files)[-1]\nprint(f\"Input: {os.path.basename(raw_file)}\")\n\ndf = pd.read_csv(raw_file)\n\n# Standardize text column name so downstream stages can rely on 'text'\nif 'content' in df.columns and 'text' not in df.columns:\n    df.rename(columns={'content': 'text'}, inplace=True)\n\n# -- Select best model by accuracy from model_meta.json --\nmodel_dirs = sorted(glob.glob(f\"{DIRS['models']}/v_*\"))\nif not model_dirs:\n    raise FileNotFoundError(f\"No model found in {DIRS['models']}/. Run Notebook 02 first.\")\n\nbest_model = None\nbest_acc = -1\n\nfor d in model_dirs:\n    meta_path = os.path.join(d, \"model_meta.json\")\n    if os.path.exists(meta_path):\n        with open(meta_path) as f:\n            meta = json.load(f)\n        acc = meta.get(\"accuracy\", 0)\n        if acc > best_acc:\n            best_acc = acc\n            best_model = d\n\n# Fallback: if no model has metadata, use the newest directory\nif best_model is None:\n    best_model = model_dirs[-1]\n\nif best_acc >= 0:\n    print(f\"Model: {os.path.basename(best_model)} (acc={best_acc:.4f})\")\nelse:\n    print(f\"Model: {os.path.basename(best_model)} (no metadata)\")\n\n# -- Run inference --\nclassifier = pipeline(\"text-classification\", model=best_model, tokenizer=best_model, top_k=None)\nprint(f\"Predicting {len(df)} rows...\")\n\ntexts = df['text'].astype(str).tolist()\npreds = classifier(texts, truncation=True, max_length=MAX_LENGTH)\n\nresults = []\nfor p in preds:\n    best = max(p, key=lambda x: x['score'])\n    results.append({\"predicted_label\": best['label'], \"confidence\": best['score']})\n\ndf_pred = pd.concat([df, pd.DataFrame(results)], axis=1)\n\n# -- Split by confidence threshold --\ngood_df = df_pred[df_pred['confidence'] >= CONFIDENCE_THRESHOLD]\nbad_df = df_pred[df_pred['confidence'] < CONFIDENCE_THRESHOLD]\n\n# Auto-pass → daily workspace\ngood_path = f\"{DIRS['workspace']}/{TODAY}_{CURRENT_PROJECT}_auto.csv\"\ngood_df.to_csv(good_path, index=False)\n\n# Low confidence → review queue\nif not bad_df.empty:\n    bad_path = f\"{DIRS['review']}/{TODAY}_{CURRENT_PROJECT}_review.csv\"\n    bad_df.to_csv(bad_path, index=False)\n    print(f\"Low confidence: {len(bad_df)} rows -> {os.path.basename(bad_path)}\")\nelse:\n    print(\"All rows passed confidence threshold.\")\n\nprint(f\"STAGE 1 complete. Auto: {len(good_df)}, Review: {len(bad_df)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61f860-1077-4ba6-b0c0-ee1abceb083c",
   "metadata": {},
   "outputs": [],
   "source": "# ==========================================\n# STAGE 2: Merge & Deliver\n#\n# Merges auto-pass data with human corrections. Supports partial\n# review: rows with `label` filled = reviewed, rows with `label`\n# empty = unreviewed (uses model prediction as fallback).\n#\n# Input: Stage 1's auto-pass (07_workspace) always has 'predicted_label'.\n#        Corrected file (07_workspace) has user-added 'label' column.\n#\n# Outputs:\n#   - 04_gold_standard: only human-reviewed rows (training fuel)\n#   - 03_manual_review: unreviewed rows written back as pending\n#   - 08_client_reports: complete report with label_source tracking\n# ==========================================\n\nimport pandas as pd\nimport glob\nimport os\nimport datetime\nfrom config import DIRS\n\nCURRENT_PROJECT = \"cold_start\"\nTODAY = datetime.date.today().strftime(\"%Y%m%d\")\n\nprint(f\"STAGE 2: Merging project [{CURRENT_PROJECT}]...\")\n\n# -- 1. Load auto-pass data from daily workspace --\ndf_auto = pd.DataFrame()\nauto_pattern = f\"{DIRS['workspace']}/{TODAY}_{CURRENT_PROJECT}_auto.csv\"\nauto_files = glob.glob(auto_pattern)\n\nif auto_files:\n    df_auto = pd.read_csv(auto_files[0])\n    # Rename model prediction to canonical 'label' for report merging\n    df_auto.rename(columns={'predicted_label': 'label'}, inplace=True)\n    df_auto['label_source'] = 'auto'\n    print(f\"  Auto-pass: {len(df_auto)} rows\")\nelse:\n    print(\"  Auto-pass: not found\")\n\n# -- 2. Load corrected file and split by review status --\ndf_reviewed = pd.DataFrame()\ndf_pending = pd.DataFrame()\n\nfixed_pattern = f\"{DIRS['workspace']}/*{CURRENT_PROJECT}*corrected*.csv\"\nfixed_files = glob.glob(fixed_pattern)\n\nif fixed_files:\n    latest_fixed = sorted(fixed_files)[-1]\n    df_corrected = pd.read_csv(latest_fixed)\n    print(f\"  Corrected file: {len(df_corrected)} rows ({os.path.basename(latest_fixed)})\")\n\n    # Split: label filled = human-reviewed, label empty = unreviewed\n    if 'label' in df_corrected.columns:\n        mask_reviewed = df_corrected['label'].notna() & (df_corrected['label'].astype(str).str.strip() != '')\n        df_reviewed = df_corrected[mask_reviewed].copy()\n        df_pending = df_corrected[~mask_reviewed].copy()\n    else:\n        # No label column at all = entirely unreviewed\n        df_pending = df_corrected.copy()\n\n    print(f\"    Reviewed: {len(df_reviewed)} rows\")\n    print(f\"    Unreviewed: {len(df_pending)} rows\")\n\n    if not df_reviewed.empty:\n        df_reviewed['label_source'] = 'human'\n\n    # Fallback for unreviewed: use model prediction as label\n    if not df_pending.empty:\n        df_pending['label'] = df_pending['predicted_label']\n        df_pending['label_source'] = 'model_pending'\nelse:\n    print(\"  Corrected file: not found\")\n\n# -- 3. Gold: archive only human-reviewed rows (training fuel) --\nif not df_reviewed.empty:\n    gold_path = f\"{DIRS['gold']}/{TODAY}_{CURRENT_PROJECT}_corrected.csv\"\n    gold_df = df_reviewed[['text', 'label']].copy()\n    gold_df['gold_origin'] = 'daily_corrected'\n    gold_df['created_at'] = datetime.datetime.now().isoformat()\n    gold_df.to_csv(gold_path, index=False)\n    print(f\"  Gold: {len(gold_df)} reviewed rows archived\")\n\n# -- 4. Pending: write unreviewed rows back to review queue --\nif not df_pending.empty:\n    pending_path = f\"{DIRS['review']}/{TODAY}_{CURRENT_PROJECT}_pending.csv\"\n    # Strip temporary columns before writing back\n    cols_to_drop = ['label', 'label_source']\n    df_pending_save = df_pending.drop(columns=[c for c in cols_to_drop if c in df_pending.columns])\n    df_pending_save.to_csv(pending_path, index=False)\n    print(f\"  Pending: {len(df_pending_save)} rows written back to review queue\")\n\n# -- 5. Final report: merge all parts with label_source tracking --\nreport_parts = []\nif not df_auto.empty:\n    report_parts.append(df_auto)\nif not df_reviewed.empty:\n    report_parts.append(df_reviewed)\nif not df_pending.empty:\n    report_parts.append(df_pending)\n\nif not report_parts:\n    print(\"ERROR: No data found for report.\")\nelse:\n    # Deterministic column order: key columns first, then alphabetical\n    common_cols = set(report_parts[0].columns)\n    for part in report_parts[1:]:\n        common_cols &= set(part.columns)\n\n    priority = ['text', 'label', 'label_source', 'confidence', 'predicted_label']\n    ordered_cols = [c for c in priority if c in common_cols]\n    ordered_cols += sorted(c for c in common_cols if c not in priority)\n\n    df_final = pd.concat([p[ordered_cols] for p in report_parts], ignore_index=True)\n\n    report_path = f\"{DIRS['reports']}/{TODAY}_{CURRENT_PROJECT}_Final_Report.csv\"\n    df_final.to_csv(report_path, index=False)\n\n    src = df_final['label_source'].value_counts().to_dict()\n    print(\"-\" * 30)\n    print(f\"STAGE 2 complete: {report_path}\")\n    print(f\"  Total: {len(df_final)} rows\")\n    print(f\"  auto: {src.get('auto', 0)} | human: {src.get('human', 0)} | model_pending: {src.get('model_pending', 0)}\")\n    print(\"-\" * 30)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e462b86-22c5-4db9-9ad6-f74ce4cf1ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}